{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8af3128-8cf6-4ff1-a338-04b3644d9660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import os\n",
    "import shutil\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 資料增強與標準化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 首先將圖像大小調整為 256x256\n",
    "    transforms.CenterCrop(224),  # 然後將圖像中心裁剪為 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "base_train_dir = '../train'\n",
    "base_val_dir = '../val'\n",
    "base_test_dir = '../test'\n",
    "\n",
    "# 創建資料集\n",
    "train_dataset = datasets.ImageFolder(base_train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(base_val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(base_test_dir, transform=transform)\n",
    "\n",
    "# 創建資料加載器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e978cb5-316d-4823-abbd-6d8532830f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 訓練函數\n",
    "def train(epoch, epochs, model, train_loader, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "    \n",
    "    for step, data in enumerate(train_bar):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # 確保數據在正確的設備上\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)  # 模型輸出\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 計算準確率\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "    running_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return running_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb314b2-4fd8-4d93-b994-6ec638e6ff63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 驗證函數\n",
    "def validate(epoch, epochs, model, validate_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_num = len(validate_loader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "        for val_data in val_bar:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            outputs = model(val_images)\n",
    "            loss = loss_function(outputs, val_labels)\n",
    "            val_loss += loss.item() * val_images.size(0)\n",
    "\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            acc += torch.eq(predict_y, val_labels).sum().item()\n",
    "\n",
    "            val_bar.desc = \"valid epoch[{}/{}]\".format(epoch + 1, epochs)\n",
    "    \n",
    "    val_loss /= val_num\n",
    "    val_accurate = 100. *acc / val_num\n",
    "    return val_loss, val_accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3b5128-2111-42e0-b7b3-bb2fdc816b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "from resnet import resnet34 \n",
    "# 設定設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "# 定義 ResNet-34 模型（不使用預訓練權重）\n",
    "model = resnet34(num_classes=50, include_top=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83bc86f4-aeb2-4d1b-bcb2-759a38083e70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:3.127: 100%|██████████| 1979/1979 [02:48<00:00, 11.75it/s]\n",
      "valid epoch[1/10]: 100%|██████████| 15/15 [00:00<00:00, 18.20it/s]\n",
      "[epoch 1] train_loss: 3.284  train_accuracy: 13.047\n",
      "[epoch 1] val_loss: 2.862  val_accuracy: 18.889\n",
      "Training_Time: 169.36 seconds\n",
      "train epoch[2/10] loss:2.394: 100%|██████████| 1979/1979 [02:44<00:00, 12.02it/s]\n",
      "valid epoch[2/10]: 100%|██████████| 15/15 [00:00<00:00, 18.87it/s]\n",
      "[epoch 2] train_loss: 2.663  train_accuracy: 25.025\n",
      "[epoch 2] val_loss: 2.565  val_accuracy: 27.333\n",
      "Training_Time: 165.50 seconds\n",
      "train epoch[3/10] loss:1.698: 100%|██████████| 1979/1979 [02:45<00:00, 11.93it/s]\n",
      "valid epoch[3/10]: 100%|██████████| 15/15 [00:00<00:00, 19.90it/s]\n",
      "[epoch 3] train_loss: 2.180  train_accuracy: 36.663\n",
      "[epoch 3] val_loss: 2.090  val_accuracy: 43.333\n",
      "Training_Time: 166.74 seconds\n",
      "train epoch[4/10] loss:1.781: 100%|██████████| 1979/1979 [02:46<00:00, 11.89it/s]\n",
      "valid epoch[4/10]: 100%|██████████| 15/15 [00:00<00:00, 20.71it/s]\n",
      "[epoch 4] train_loss: 1.777  train_accuracy: 47.253\n",
      "[epoch 4] val_loss: 1.615  val_accuracy: 50.667\n",
      "Training_Time: 167.29 seconds\n",
      "train epoch[5/10] loss:1.376: 100%|██████████| 1979/1979 [02:44<00:00, 12.02it/s]\n",
      "valid epoch[5/10]: 100%|██████████| 15/15 [00:00<00:00, 19.31it/s]\n",
      "[epoch 5] train_loss: 1.481  train_accuracy: 55.051\n",
      "[epoch 5] val_loss: 1.709  val_accuracy: 52.667\n",
      "Training_Time: 165.50 seconds\n",
      "train epoch[6/10] loss:1.130: 100%|██████████| 1979/1979 [02:47<00:00, 11.81it/s]\n",
      "valid epoch[6/10]: 100%|██████████| 15/15 [00:00<00:00, 19.15it/s]\n",
      "[epoch 6] train_loss: 1.238  train_accuracy: 61.480\n",
      "[epoch 6] val_loss: 1.585  val_accuracy: 55.556\n",
      "Training_Time: 168.48 seconds\n",
      "train epoch[7/10] loss:0.910: 100%|██████████| 1979/1979 [02:43<00:00, 12.12it/s]\n",
      "valid epoch[7/10]: 100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "[epoch 7] train_loss: 1.027  train_accuracy: 67.221\n",
      "[epoch 7] val_loss: 1.471  val_accuracy: 57.111\n",
      "Training_Time: 164.21 seconds\n",
      "train epoch[8/10] loss:1.035: 100%|██████████| 1979/1979 [02:44<00:00, 12.00it/s]\n",
      "valid epoch[8/10]: 100%|██████████| 15/15 [00:00<00:00, 20.57it/s]\n",
      "[epoch 8] train_loss: 0.831  train_accuracy: 72.660\n",
      "[epoch 8] val_loss: 1.399  val_accuracy: 59.333\n",
      "Training_Time: 165.81 seconds\n",
      "train epoch[9/10] loss:0.522: 100%|██████████| 1979/1979 [02:45<00:00, 11.97it/s]\n",
      "valid epoch[9/10]: 100%|██████████| 15/15 [00:00<00:00, 20.66it/s]\n",
      "[epoch 9] train_loss: 0.638  train_accuracy: 78.459\n",
      "[epoch 9] val_loss: 1.444  val_accuracy: 60.222\n",
      "Training_Time: 166.21 seconds\n",
      "train epoch[10/10] loss:0.235: 100%|██████████| 1979/1979 [02:44<00:00, 12.05it/s]\n",
      "valid epoch[10/10]: 100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "[epoch 10] train_loss: 0.475  train_accuracy: 83.735\n",
      "[epoch 10] val_loss: 1.589  val_accuracy: 59.333\n",
      "Training_Time: 164.95 seconds\n",
      "訓練完成\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# 訓練和驗證模型\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "save_path = 'best_resnet34.pth'\n",
    "t_l, t_a = [], []\n",
    "v_l, v_a = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy = train(epoch, num_epochs, model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accurate = validate(epoch, num_epochs, model, val_loader, criterion, device)\n",
    "    t_l.append(train_loss)\n",
    "    t_a.append(train_accuracy)\n",
    "    v_l.append(val_loss)\n",
    "    v_a.append(val_accurate)\n",
    "    \n",
    "    print('[epoch %d] train_loss: %.3f  train_accuracy: %.3f' %\n",
    "            (epoch + 1, train_loss, train_accuracy))\n",
    "    print('[epoch %d] val_loss: %.3f  val_accuracy: %.3f' %\n",
    "            (epoch + 1, val_loss, val_accurate))\n",
    "    \n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    end_time = time.time()\n",
    "    print(f'Training_Time: {end_time - start_time:.2f} seconds')\n",
    "    \n",
    "print('訓練完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e840591-6845-4754-b1f3-cad492907d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6578\n"
     ]
    }
   ],
   "source": [
    "# 測試模型\n",
    "model.load_state_dict(torch.load('best_resnet34.pth'))\n",
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_running_corrects.double() / len(test_dataset)\n",
    "print(f'Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d2c8c2-a9b3-4d15-ba12-e8df2553cddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_metrics_to_file(t_l, t_a, v_l, v_a, filename='metrics.txt'):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"Train Loss:\\n\")\n",
    "        for item in t_l:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Train Accuracy:\\n\")\n",
    "        for item in t_a:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Validation Loss:\\n\")\n",
    "        for item in v_l:\n",
    "            file.write(f\"{item}\\n\")\n",
    "        \n",
    "        file.write(\"Validation Accuracy:\\n\")\n",
    "        for item in v_a:\n",
    "            file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074fd2e8-7168-4497-9e8b-663b2f83bf99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假設 t_l, t_a, v_l, v_a 已經被填充\n",
    "filename = 'resnet34.txt'\n",
    "save_metrics_to_file(t_l, t_a, v_l, v_a, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04999d6c-732c-41e4-a659-3445f1eff95e",
   "metadata": {},
   "source": [
    "# import revised-alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8bef1c-932d-4e07-b6b8-6d6e83881654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "from revised_alexnet import AlexNet\n",
    "# 設定設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "# 定義 revised_alexnet 模型（不使用預訓練權重）\n",
    "model = AlexNet(num_classes=50)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d7b544-03be-4afa-967a-faa5f0e59618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:3.924: 100%|██████████| 1979/1979 [01:49<00:00, 18.15it/s]\n",
      "valid epoch[1/10]: 100%|██████████| 15/15 [00:00<00:00, 23.67it/s]\n",
      "[epoch 1] train_loss: 3.911  train_accuracy: 1.832\n",
      "[epoch 1] val_loss: 3.913  val_accuracy: 2.000\n",
      "Training_Time: 109.73 seconds\n",
      "train epoch[2/10] loss:3.900: 100%|██████████| 1979/1979 [01:47<00:00, 18.40it/s]\n",
      "valid epoch[2/10]: 100%|██████████| 15/15 [00:00<00:00, 24.59it/s]\n",
      "[epoch 2] train_loss: 3.910  train_accuracy: 1.931\n",
      "[epoch 2] val_loss: 3.914  val_accuracy: 2.000\n",
      "Training_Time: 108.18 seconds\n",
      "train epoch[3/10] loss:3.901: 100%|██████████| 1979/1979 [01:44<00:00, 18.99it/s]\n",
      "valid epoch[3/10]: 100%|██████████| 15/15 [00:00<00:00, 25.13it/s]\n",
      "[epoch 3] train_loss: 3.910  train_accuracy: 1.911\n",
      "[epoch 3] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 104.81 seconds\n",
      "train epoch[4/10] loss:3.915: 100%|██████████| 1979/1979 [01:46<00:00, 18.52it/s]\n",
      "valid epoch[4/10]: 100%|██████████| 15/15 [00:00<00:00, 25.60it/s]\n",
      "[epoch 4] train_loss: 3.910  train_accuracy: 1.843\n",
      "[epoch 4] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 107.43 seconds\n",
      "train epoch[5/10] loss:3.900: 100%|██████████| 1979/1979 [01:45<00:00, 18.73it/s]\n",
      "valid epoch[5/10]: 100%|██████████| 15/15 [00:00<00:00, 25.09it/s]\n",
      "[epoch 5] train_loss: 3.910  train_accuracy: 1.893\n",
      "[epoch 5] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 106.25 seconds\n",
      "train epoch[6/10] loss:3.903: 100%|██████████| 1979/1979 [01:45<00:00, 18.84it/s]\n",
      "valid epoch[6/10]: 100%|██████████| 15/15 [00:00<00:00, 24.72it/s]\n",
      "[epoch 6] train_loss: 3.910  train_accuracy: 1.928\n",
      "[epoch 6] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 105.65 seconds\n",
      "train epoch[7/10] loss:3.921: 100%|██████████| 1979/1979 [01:45<00:00, 18.80it/s]\n",
      "valid epoch[7/10]: 100%|██████████| 15/15 [00:00<00:00, 23.72it/s]\n",
      "[epoch 7] train_loss: 3.910  train_accuracy: 1.856\n",
      "[epoch 7] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 105.92 seconds\n",
      "train epoch[8/10] loss:3.900: 100%|██████████| 1979/1979 [01:44<00:00, 18.98it/s]\n",
      "valid epoch[8/10]: 100%|██████████| 15/15 [00:00<00:00, 25.98it/s]\n",
      "[epoch 8] train_loss: 3.910  train_accuracy: 1.917\n",
      "[epoch 8] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 104.87 seconds\n",
      "train epoch[9/10] loss:3.911: 100%|██████████| 1979/1979 [01:46<00:00, 18.51it/s]\n",
      "valid epoch[9/10]: 100%|██████████| 15/15 [00:00<00:00, 23.40it/s]\n",
      "[epoch 9] train_loss: 3.910  train_accuracy: 1.843\n",
      "[epoch 9] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 107.57 seconds\n",
      "train epoch[10/10] loss:3.905: 100%|██████████| 1979/1979 [01:46<00:00, 18.63it/s]\n",
      "valid epoch[10/10]: 100%|██████████| 15/15 [00:00<00:00, 23.64it/s]\n",
      "[epoch 10] train_loss: 3.910  train_accuracy: 1.867\n",
      "[epoch 10] val_loss: 3.915  val_accuracy: 2.000\n",
      "Training_Time: 106.89 seconds\n",
      "訓練完成\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# 訓練和驗證模型\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "save_path = 'best_re_alexnet.pth'\n",
    "t_l, t_a = [], []\n",
    "v_l, v_a = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy = train(epoch, num_epochs, model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accurate = validate(epoch, num_epochs, model, val_loader, criterion, device)\n",
    "    t_l.append(train_loss)\n",
    "    t_a.append(train_accuracy)\n",
    "    v_l.append(val_loss)\n",
    "    v_a.append(val_accurate)\n",
    "    \n",
    "    print('[epoch %d] train_loss: %.3f  train_accuracy: %.3f' %\n",
    "            (epoch + 1, train_loss, train_accuracy))\n",
    "    print('[epoch %d] val_loss: %.3f  val_accuracy: %.3f' %\n",
    "            (epoch + 1, val_loss, val_accurate))\n",
    "    \n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    end_time = time.time()\n",
    "    print(f'Training_Time: {end_time - start_time:.2f} seconds')\n",
    "    \n",
    "print('訓練完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c81e5fd-e74b-49a3-be5d-b0a525216b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假設 t_l, t_a, v_l, v_a 已經被填充\n",
    "filename = 're_alexnet.txt'\n",
    "save_metrics_to_file(t_l, t_a, v_l, v_a, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81216291-f202-4b92-89af-2be95af0335d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (710690352.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    from res_attention import ResidualAttentionModel()\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from res_attention import ResidualAttentionModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "# 定義 revised_alexnet 模型（不使用預訓練權重）\n",
    "model = ResidualAttentionModel(num_classes=50)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc201e6-92d0-40f1-8d2f-4e3bc4bf60c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:3.934: 100%|██████████| 1979/1979 [01:50<00:00, 17.95it/s]\n",
      "valid epoch[1/10]: 100%|██████████| 15/15 [00:00<00:00, 22.95it/s]\n",
      "[epoch 1] train_loss: 3.911  train_accuracy: 2.023\n",
      "[epoch 1] val_loss: 3.914  val_accuracy: 2.000\n",
      "Training_Time: 110.99 seconds\n",
      "train epoch[2/10] loss:3.897: 100%|██████████| 1979/1979 [01:47<00:00, 18.37it/s]\n",
      "valid epoch[2/10]: 100%|██████████| 15/15 [00:00<00:00, 25.18it/s]\n",
      "[epoch 2] train_loss: 3.910  train_accuracy: 1.909\n",
      "[epoch 2] val_loss: 3.914  val_accuracy: 2.000\n",
      "Training_Time: 108.32 seconds\n",
      "train epoch[3/10] loss:3.898: 100%|██████████| 1979/1979 [01:47<00:00, 18.36it/s]\n",
      "valid epoch[3/10]: 100%|██████████| 15/15 [00:00<00:00, 24.56it/s]\n",
      "[epoch 3] train_loss: 3.910  train_accuracy: 1.927\n",
      "[epoch 3] val_loss: 3.914  val_accuracy: 2.000\n",
      "Training_Time: 108.41 seconds\n",
      "train epoch[4/10] loss:3.908:  61%|██████    | 1202/1979 [01:04<00:41, 18.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     12\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 13\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train(epoch, num_epochs, model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     14\u001b[0m     val_loss, val_accurate \u001b[38;5;241m=\u001b[39m validate(epoch, num_epochs, model, val_loader, criterion, device)\n\u001b[0;32m     15\u001b[0m     t_l\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, epochs, model, train_loader, optimizer, loss_function, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_bar):\n\u001b[0;32m     12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     13\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 確保數據在正確的設備上\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\Lib\\site-packages\\torchvision\\transforms\\functional.py:175\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# 訓練和驗證模型\n",
    "num_epochs = 10\n",
    "best_acc = 0.0\n",
    "save_path = 'best_re_resattention.pth'\n",
    "t_l, t_a = [], []\n",
    "v_l, v_a = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy = train(epoch, num_epochs, model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accurate = validate(epoch, num_epochs, model, val_loader, criterion, device)\n",
    "    t_l.append(train_loss)\n",
    "    t_a.append(train_accuracy)\n",
    "    v_l.append(val_loss)\n",
    "    v_a.append(val_accurate)\n",
    "    \n",
    "    print('[epoch %d] train_loss: %.3f  train_accuracy: %.3f' %\n",
    "            (epoch + 1, train_loss, train_accuracy))\n",
    "    print('[epoch %d] val_loss: %.3f  val_accuracy: %.3f' %\n",
    "            (epoch + 1, val_loss, val_accurate))\n",
    "    \n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    end_time = time.time()\n",
    "    print(f'Training_Time: {end_time - start_time:.2f} seconds')\n",
    "    \n",
    "print('訓練完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63e60c-5c11-4c26-9507-92d68c95a6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
